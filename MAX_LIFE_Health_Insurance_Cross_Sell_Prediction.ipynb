{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssinghvishal/MAX-LIFE-Health-Insurance-Cross-Sell-Prediction/blob/main/MAX_LIFE_Health_Insurance_Cross_Sell_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MAX-LIFE-Health-Insurance-Cross-Sell-Prediction**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual\n",
        "##### **Completed by** - Vishal singh"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insurance industry is characterized by its reliance on statistical models and predictive analytics to optimize business strategies and increase revenue. In this project, we focus on a Health Insurance company that aims to expand its offerings by predicting whether their existing health insurance policyholders would be interested in purchasing vehicle insurance. The central objective is to develop a machine learning model that can accurately classify customers based on their likelihood of buying vehicle insurance, thereby enabling the company to tailor its marketing and communication strategies effectively.\n",
        "\n",
        "Insurance policies, whether for health or vehicles, operate on the principle of risk sharing and compensation. Customers pay a regular premium in exchange for a guarantee of compensation in the event of specific losses, damages, or illnesses. The premium amounts are calculated based on the probability of the insured event occurring, which is where the company's risk assessment capabilities are crucial. By leveraging demographic data, vehicle information, and policy details, we aim to identify patterns and predictors of customer behavior regarding vehicle insurance uptake.\n",
        "\n",
        "The dataset provided for this project includes various features: demographics such as gender, age, and region; vehicle details including vehicle age and damage history; and policy information covering premium amounts and sourcing channels. These features will serve as the input variables for our classification model. Our target variable is a binary indicator of whether a customer is interested in vehicle insurance.\n",
        "\n",
        "To build a robust predictive model, the project will follow a structured approach encompassing several key steps. Initially, we will perform exploratory data analysis (EDA) to understand the dataset, identify missing values, and visualize key trends. This will be followed by data preprocessing, which includes handling missing values, encoding categorical variables, and scaling numerical features.\n",
        "\n",
        "Subsequently, we will split the data into training and testing sets to ensure that our model's performance can be evaluated on unseen data. Various classification algorithms will be explored, including logistic regression, decision trees, random forests, gradient boosting machines, and neural networks. Each model's performance will be assessed using metrics such as accuracy, precision, recall, F1 score, and the area under the receiver operating characteristic curve (AUC-ROC).\n",
        "\n",
        "Hyperparameter tuning will be conducted to optimize the models, and cross-validation techniques will be employed to ensure robustness and prevent overfitting. The final model will be selected based on its predictive performance and interpretability.\n",
        "\n",
        "In conclusion, this project aims to harness the power of machine learning to support the insurance company in its strategic decision-making process, ultimately leading to better business outcomes and customer engagement."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insurance industry heavily relies on statistical models and predictive analytics to optimize business strategies and enhance revenue. This project focuses on a Health Insurance company seeking to expand its offerings by predicting whether their existing health insurance policyholders would be interested in purchasing vehicle insurance. The primary objective is to build a machine learning model capable of accurately classifying customers based on their likelihood of buying vehicle insurance. This enables the company to effectively tailor its marketing and communication strategies."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, f1_score, precision_score, recall_score, classification_report\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
        "import scipy.stats as stats\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9wc4iPNVFE90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "filepath='/content/drive/MyDrive/Ml model 2/TRAIN-HEALTH INSURANCE CROSS SELL PREDICTION.csv'\n",
        "df=pd.read_csv(filepath , encoding = 'ISO-8859-1')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dataset Rows & Columns count\n",
        "\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Dataset Duplicate Value Count\n",
        "\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(df.isnull())"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The client, a Health Insurance company, is seeking to enhance its product offerings by predicting whether its current health insurance policyholders will be interested in purchasing vehicle insurance.\n",
        "\n",
        "The dataset contains 381109 rows and 12 columns. It have 0 duplicates and 0 null values, overall the quality of dataset is really good."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **id**: Unique ID for customer\n",
        "\n",
        "* **Gender**: Male/Female\n",
        "* **Age:** Age of Customer\n",
        "* **Driving License**: Customer has DL or not\n",
        "* **egion_code**: Unique code for the region of the customer\n",
        "* **Previously_insured**: Customer already has vehicle insuarance or not\n",
        "* **Vehicle_age**: Age of vehicle\n",
        "* **Vehicle_damage**: Past damages present or not\n",
        "* **Annual_premium**: The amount customer needs to pay as premium\n",
        "* **PolicySalesChanne**: Anonymized Code for the channel of outreaching to customer *ie. Different Agents, Over Mail, Over Phone, In person, etc\n",
        "* **Vintage**: Number of Days, Customer has been associated with the company\n",
        "* **Response** : Customer is interested or not"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "numerical_df = df.select_dtypes(include=np.number)\n",
        "numerical_df.head()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# creating a categorical dataframe\n",
        "categorical_df = df.select_dtypes(include=object)\n",
        "categorical_df.head()"
      ],
      "metadata": {
        "id": "NBPq5McCIjuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this dataset there is nothing to do much for data wrangling. But I used dataframes 'select_dtypes' method to create numerical and categorical dataframes."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "print(df['Age'].value_counts().reset_index())\n",
        "\n",
        "# the histplot is used to understand the distribution of contionus variable\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.histplot(x='Age',data=df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram was chosen to visualize the distribution of customer ages because it effectively represents the frequency of occurrences within specific intervals or bins. Histograms are particularly useful for understanding the shape and spread of continuous data, such as age, and can highlight patterns such as skewness, modality, and the presence of outliers. In this context, visualizing the age distribution of customers helps in identifying the age groups that are most prevalent among the policyholders, which is crucial for tailoring marketing strategies and product offerings."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The majority of the policyholders fall within the younger age brackets, particularly between the ages of 21 and 25, with the highest counts being for ages 24, 23, and 22.There is a noticeable decline in the number of policyholders as age increases beyond 25 and another small peak between 42-48.Very few policyholders are above the age of 80, indicating that older customers are significantly less common."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "Yes, the insights gained from the age distribution histogram can positively impact the business in several ways:\n",
        "\n",
        "Understanding that the majority of policyholders are young adults can help the company design and implement targeted marketing campaigns. For instance, promotional efforts can be directed towards platforms and channels that are popular among younger demographics.\n",
        "\n",
        "nowing the age distribution allows for the customization of insurance products to better meet the needs and preferences of younger customers. For example, offering bundled insurance products that combine health and vehicle insurance at competitive rates may appeal more to this age group.\n",
        "\n",
        "Negative impact: The heavy concentration of young policyholders might indicate an over-reliance on this demographic. If market conditions change or this age group becomes less interested in vehicle insurance, it could negatively impact growth. Diversifying the age range of policyholders could mitigate this risk."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Chart - 2 visualization code\n",
        "\n",
        "print(df['Response'].value_counts().reset_index())\n",
        "\n",
        "# the pie chart is used to visualize the distribution of whole\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.title('Response')\n",
        "plt.pie(df['Response'].value_counts(), startangle=90, labels=['Not Interested(%)','Interested(%)'], autopct='%1.1f%%', colors=sns.color_palette('Set2'))\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart was chosen to visualize the distribution of the response variable because it provides a clear and intuitive representation of the proportions within a categorical dataset. Pie charts are particularly effective for displaying the relative sizes of different categories as parts of a whole. In this context, visualizing the response variable (whether a customer is interested in vehicle insurance) helps in understanding the overall distribution of interest and disinterest among policyholders."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A significant majority of the policyholders (334,399 or approximately 87.8%) are not interested in vehicle insurance.\n",
        "\n",
        "A smaller proportion of policyholders (46,710 or approximately 12.2%) are interested in vehicle insurance."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "Yes, the insights gained from the pie chart can positively impact the business in several ways:\n",
        "\n",
        "Understanding that a smaller proportion of the policyholders are currently interested in vehicle insurance allows the company to tailor its marketing strategies more effectively. By identifying and targeting the factors that differentiate the interested group, the company can design campaigns that might convert more disinterested customers.\n",
        "\n",
        "The insights help in allocating resources more efficiently. The company can focus more on the factors that influence interest in vehicle insurance and improve or highlight those aspects in their offerings.\n",
        "\n",
        "Negative impact: The overwhelming majority of disinterested policyholders might indicate underlying issues such as lack of awareness, perceived irrelevance of vehicle insurance, or unattractive product features. If these issues are not addressed, the company may struggle to increase its vehicle insurance customer base."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "\n",
        "print(df['Gender'].value_counts().reset_index())\n",
        "\n",
        "# the pie chart is used to visualize the distribution of whole\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.title('Gender')\n",
        "plt.pie(df['Gender'].value_counts(), startangle=90, labels=['Male','Female'], autopct='%1.1f%%', colors=sns.color_palette('Set2'))\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart was chosen to visualize the gender distribution because it effectively demonstrates the proportional breakdown of categorical data. Pie charts are particularly useful for displaying the relative sizes of categories as parts of a whole, making it easy to see the composition of the dataset at a glance. In this context, visualizing the gender distribution of policyholders helps in understanding the demographic makeup of the customer base, which is crucial for designing targeted marketing and product strategies."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows that male policyholders constitute a slight majority, with 206,089 male customers, representing approximately 54.1% of the total.\n",
        "\n",
        "Female policyholders account for 175,020 of the total, making up around 45.9%."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "Yes, the insights gained from the pie chart can positively impact the business in several ways:\n",
        "\n",
        "The company can use this gender information to customize its insurance products. For example, they can offer vehicle insurance plans that address specific needs or preferences that might differ between male and female customers.The relatively balanced gender distribution suggests that marketing resources can be allocated in a way that addresses both male and female customers' preferences, ensuring a comprehensive approach to customer engagement.\n",
        "\n",
        "Negative impact: If the slight male majority is due to a gender bias in marketing or product offerings, the company might be unintentionally neglecting opportunities to attract more female customers. Ensuring gender-neutral marketing and product development can help in mitigating this risk."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Chart - 4 visualization code\n",
        "\n",
        "print(df['Driving_License'].value_counts().reset_index())\n",
        "\n",
        "# the pie chart is used to visualize the distribution of whole\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.title('Driving_License')\n",
        "plt.pie(df['Driving_License'].value_counts(), startangle=140, labels=[\"Have DL\",\"Don't have DL \"], autopct='%1.1f%%', colors=sns.color_palette('Set2'))\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart was chosen to visualize the distribution of the Driving_License variable because it clearly shows the proportions of policyholders who do and do not have a driving license. Pie charts are effective for displaying the relative sizes of different categories as parts of a whole, making it easy to understand the composition of the dataset. In this context, visualizing whether policyholders have a driving license helps in understanding the potential market for vehicle insurance."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A vast majority of policyholders (380,297 or approximately 99.8%) have a driving license.\n",
        "\n",
        "Only a small fraction of policyholders (812 or approximately 0.2%) do not have a driving license."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "Yes, the insights gained from the pie chart can positively impact the business in several ways:\n",
        "\n",
        "he fact that nearly all policyholders have a driving license indicates a high potential market for vehicle insurance. Since having a driving license is a prerequisite for driving, these customers are likely candidates for vehicle insurance products.\n",
        "\n",
        "The company can confidently focus its marketing efforts for vehicle insurance on the vast majority of its policyholders, as they are all licensed drivers and thus potential buyers of vehicle insurance."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "\n",
        "print(df['Annual_Premium'].value_counts().reset_index())\n",
        "\n",
        "# the histplot is used to understand the distribution of contionus variable\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title('Annual_Premium')\n",
        "sns.histplot(x='Annual_Premium',data=df)\n",
        "plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram was chosen to visualize the distribution of the Annual_Premium variable because it effectively displays the frequency of premium values within specified intervals or bins. Histograms are particularly useful for understanding the shape and spread of continuous data, such as annual premium amounts, and can highlight patterns such as skewness, modality, and the presence of outliers. In this context, visualizing the distribution of annual premiums helps in identifying common premium amounts and understanding the variability in customer payments."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The majority of policyholders pay lower annual premiums, with a particularly high concentration around the 2630 mark.\n",
        "\n",
        "The distribution of annual premiums is highly skewed to the right, indicating that most customers pay relatively low premiums, but there are some who pay significantly higher amounts.\n",
        "\n",
        "There are a few outliers with very high premium amounts, such as 143,525, indicating a wide range of premium values among policyholders."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "Yes, the insights gained from the histogram can positively impact the business in several ways:\n",
        "\n",
        "Understanding that most policyholders pay lower premiums allows the company to optimize its pricing strategy. It can consider introducing more affordable insurance plans to attract price-sensitive customers or offering tiered pricing to cater to different income segments.\n",
        "\n",
        "The presence of a few high premium outliers suggests there are customers willing to pay more for enhanced coverage. The company can identify these customers and target them with upselling opportunities, such as additional coverage options or bundled insurance products. The histogram helps in segmenting customers based on their premium payments. The company can tailor its marketing and product development efforts to address the needs of different segments, ensuring a more personalized approach."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "print(df['Policy_Sales_Channel'].value_counts().reset_index())\n",
        "\n",
        "# the histplot is used to understand the distribution of contionus variable\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title('Policy_Sales_Channel')\n",
        "sns.histplot(x='Policy_Sales_Channel',data=df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram was chosen to visualize the distribution of the Policy_Sales_Channel variable because it effectively represents the frequency of policy sales through different channels. Histograms are particularly useful for understanding the spread and concentration of categorical data that has been converted into numerical form. In this context, visualizing the distribution of policy sales channels helps in identifying the most and least used channels, which is crucial for optimizing marketing and sales strategies.."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A few sales channels dominate the distribution, with channels 152, 26, and 124 being the most frequently used. Channel 152 alone accounts for a significant proportion of the sales, with 134,784 policies sold through it.\n",
        "\n",
        "The distribution of sales across channels is highly skewed, indicating that a small number of channels are responsible for the majority of sales.\n",
        "\n",
        "There are numerous channels that are rarely used, with many channels having only a handful of policies sold through them."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "Yes, the insights gained from the histogram can positively impact the business in several ways:\n",
        "\n",
        "Knowing which sales channels are the most effective allows the company to allocate resources more efficiently. Marketing and support efforts can be focused on the high-performing channels (152, 26, and 124) to maximize sales.\n",
        "\n",
        "The company can analyze why certain channels perform better than others. This analysis can lead to insights that can be applied to improve the performance of underutilized channels.\n",
        "\n",
        "The company might neglect the lower-performing channels without understanding their potential. Some channels might be underutilized due to lack of support or awareness. Proper analysis and strategic efforts might turn these channels into valuable assets."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Chart - 7 visualization code\n",
        "\n",
        "print(df['Previously_Insured'].value_counts().reset_index())\n",
        "\n",
        "# the pie chart is used to visualize the distribution of whole\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.title('Previously_Insured')\n",
        "plt.pie(df['Previously_Insured'].value_counts(), startangle=90, labels=['Previously_not_Insured','Previously_Insured'], autopct='%1.1f%%', colors=sns.color_palette('Set2'))\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart was chosen to visualize the distribution of the Previously_Insured variable because it effectively demonstrates the proportions of policyholders who were previously insured versus those who were not. Pie charts are particularly useful for displaying the relative sizes of categories as parts of a whole, making it easy to understand the composition of the dataset at a glance. In this context, visualizing the previously insured status of policyholders helps in understanding their insurance history, which is crucial for tailoring marketing strategies and insurance products."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution between policyholders who were previously insured (174,628 or approximately 45.8%) and those who were not (206,481 or approximately 54.2%) is fairly balanced.A slightly larger portion of policyholders did not have previous insurance, which indicates a substantial market segment that might be new to insurance products."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "Yes, the insights gained from the pie chart can positively impact the business in several ways:\n",
        "\n",
        "Understanding the nearly equal distribution allows the company to design targeted marketing strategies. For instance, campaigns for previously uninsured customers can focus on educating them about the benefits of vehicle insurance and addressing any barriers to entry they might perceive.\n",
        "\n",
        "The company can develop and promote products that cater specifically to the needs of previously uninsured customers, such as introductory offers, easy enrollment processes, and educational resources.\n",
        "\n",
        "For customers who were previously insured, the company can emphasize loyalty programs, enhanced coverage options, and competitive pricing to encourage them to switch and stay with their insurance products.\n",
        "\n",
        "The slightly larger group of previously uninsured customers might be more challenging to convert, as they might lack familiarity with insurance products or have perceived barriers to purchasing insurance. If not addressed properly, efforts to convert these customers might yield lower returns."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "\n",
        "print(df['Vehicle_Damage'].value_counts().reset_index())\n",
        "\n",
        "# the pie chart is used to visualize the distribution of whole\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.title('Vehicle_Damage')\n",
        "plt.pie(df['Vehicle_Damage'].value_counts(), startangle=90, labels=['Vehicle Damaged','Vehicle not Damaged'], autopct='%1.1f%%', colors=sns.color_palette('Set2'))\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pie charts are particularly useful for displaying the relative sizes of categories as parts of a whole, making it easy to understand the composition of the dataset at a glance."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution between policyholders who have experienced vehicle damage (192,413 or approximately 50.5%) and those who have not (188,696 or approximately 49.5%) is almost equal."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "Yes, the insights gained from the pie chart can positively impact the business in several ways:\n",
        "\n",
        "Risk Assessment and Pricing: Understanding that half of the policyholders have experienced vehicle damage allows the company to refine its risk assessment models and adjust premium pricing accordingly. Higher premiums might be warranted for customers with a history of vehicle damage to cover potential future claims.\n",
        "\n",
        "No, there are no such insights that lead to negative growth."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "\n",
        "Vehicle_age = df['Vehicle_Age'].value_counts().reset_index()\n",
        "print(Vehicle_age)\n",
        "\n",
        "# the barplot is used to visualize the distribution of categorical variable\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title('Vehicle_Age')\n",
        "sns.barplot(y='Vehicle_Age',x='count',data=Vehicle_age, palette='Set2')\n",
        "plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Horizontal bar charts are particularly effective for comparing the sizes of different categories side by side, making it easy to understand which categories have higher or lower counts. In this context, visualizing the distribution of vehicle age helps in understanding the age profile o"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-2 Years: The majority of insured vehicles fall within the 1-2 year age range, with a count of 200,316.\n",
        "\n",
        "There is also a significant portion of vehicles that are less than 1 year old, with a count of 164,786.\n",
        "\n",
        "A relatively small segment of vehicles is older than 2 years, with a count of 16,007."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "Yes, the insights gained from the horizontal bar chart can positively impact the business in several ways:\n",
        "\n",
        "Understanding that the majority of vehicles are relatively new (1-2 years old) allows the company to develop tailored insurance products that cater specifically to the needs of newer vehicle owners. For instance, offering features like new car replacement coverage or enhanced roadside assistance can appeal to this segment.\n",
        "\n",
        "No, there are no such insights that lead to negative growth."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "\n",
        "# countplot is useful for visualizing the distribution of categorical data and identifying the most common categories.\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.countplot(x='Vehicle_Damage',hue='Response',data=df)\n",
        "plt.title('Vehicle_Damage Vs Response')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used countplot because the countplot is used to represent the occurrence(counts) of the observation present in the categorical variable."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see from the above plot that those people whose vehicle is damaged are taking insurence more in comparison those who's vehicle are not damaged."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "Yes, the gained insights help to create a positive impact on business because on the basis of above analysis, we can target those people whose vehicles are damaged.\n",
        "\n",
        "No, there are no such insights that lead to negative growth."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "\n",
        "# countplot is useful for visualizing the distribution of categorical data and identifying the most common categories.\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.countplot(x='Vehicle_Age',hue='Response',data=df)\n",
        "plt.title('Vehicle age Vs Response',fontsize=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used countplot because the countplot is used to represent the occurrence(counts) of the observation present in the categorical variable."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plot wer can see that the customers whose vehicle age is 1-2 years are taking more insuarnce and their total volume is also greater than other two."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "Yes there are positive insights we can target the customers more with vehicle age of 1-2 and <1 years.\n",
        "\n",
        "No, there are no such insights that lead to negative growth."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12  Correlation Heatmap"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "\n",
        " # plotting correalation heatmap\n",
        "corr = numerical_df.corr()\n",
        "\n",
        "cmap = sns.diverging_palette(5, 250, as_cmap=True)\n",
        "plt.figure(figsize=(13, 8))\n",
        "sns.heatmap(corr, cmap=cmap, annot=True, fmt=\".2f\")\n",
        "plt.title('Pearson correlation of Features',fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heatmaps use color gradients to represent different levels of correlation, making it easy to identify patterns and correlations at a glance. In this context, visualizing the correlations between features helps in understanding which variables are interrelated, which is crucial for feature selection."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a strong negative correlation between Age and Policy_Sales_Channel (-0.58),indicating that younger customers tend to purchase policies through different channels compared to older customers.\n",
        "\n",
        "There is a moderate negative correlation between Previously_Insured and Response (-0.34), suggesting that customers who were previously insured are less likely to be interested in vehicle insurance.\n",
        "\n",
        "There is a positive correlation between Age and Response (0.11), suggesting that older customers are more likely to be interested in vehicle insurance.\n",
        "\n",
        "Most other correlations are relatively weak, indicating minimal linear relationships between those pairs of variables."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Pair Plot visualization code\n",
        "# Commenting this code takes forever to run\n",
        "\n",
        "'''sns.pairplot(df,hue='Response')\n",
        "plt.show()'''\n",
        "\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pair plot, also known as a scatterplot matrix, is a matrix of graphs that enables the visualization of the relationship between each pair of variables in a dataset"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph above shows how each feature is distributed in respect to other features. Since many features have binary values, we cannot see a good relationship with other features. Due to more unique values,some of the features are uniformly distributed. The premium feature has a skewed relationship with the other features since it is skewed to the right."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Three questions for Hypothesis testing\n",
        "\n",
        "1.If customer buying Health insurance then atleast 220 days, customer has been assosiated with the company.\n",
        "\n",
        "2.If the customer buying the insuarance then the age will be at most 55 years.\n",
        "\n",
        "3.The customer will buy the Insuarance if the Annual Premium is at most 35000"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis: N >= 220\n",
        "\n",
        "Alternate Hypothesis : N < 220\n",
        "\n",
        "Test Type: Left Tailed Test"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# Extract the 'vintage' column\n",
        "vintage_data = df['Vintage']\n",
        "\n",
        "# Null Hypothesis: N >= 220\n",
        "# Alternate Hypothesis: N < 220\n",
        "# Test Type: Left Tailed Test\n",
        "\n",
        "mu = 220  # population mean according to the null hypothesis\n",
        "alpha = 0.05  # significance level\n",
        "\n",
        "\n",
        "# Calculate the sample mean and standard deviation\n",
        "sample_mean = vintage_data.mean()\n",
        "sample_std = vintage_data.std(ddof=1)\n",
        "n = len(vintage_data)\n",
        "\n",
        "# Perform the one-sample t-test\n",
        "t_statistic, p_value = stats.ttest_1samp(vintage_data, mu)\n",
        "\n",
        "# Since it's a left-tailed test, we adjust the p-value\n",
        "p_value_left_tailed = p_value / 2 if t_statistic < 0 else 1 - (p_value / 2)\n",
        "\n",
        "# Print the results\n",
        "print(\"Sample mean:\", sample_mean)\n",
        "print(\"Sample standard deviation:\", sample_std)\n",
        "print(\"Sample size:\", n)\n",
        "print(\"T-statistic:\", t_statistic)\n",
        "print(\"Left-tailed p-value:\", p_value_left_tailed)\n",
        "\n",
        "# Decision based on the p-value\n",
        "if p_value_left_tailed < alpha:\n",
        "    print(\"Reject the null hypothesis: There is evidence that N < 220.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: There is not enough evidence that N < 220.\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical test performed is a one-sample t-test. Specifically, it is used to test whether the mean of a sample is significantly different from a known or hypothesized population mean. In this case, the t-test is used to compare the sample mean of the 'Vintage' column to the hypothesized population mean of 220 days."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis Testing: The null hypothesis (H0) states that the population mean N is greater than or equal to 220 days, while the alternative hypothesis (H1) states that N is less than 220 days. The one-sample t-test is appropriate for testing hypotheses about the mean of a single sample against a known or hypothesized population mean. Direction of the Test: This is a left-tailed test, as the alternative hypothesis suggests that the mean is less than 220. The one-sample t-test can be easily adapted to perform a one-tailed test by adjusting the p-value calculation."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis: N <= 55\n",
        "\n",
        "Alternate Hypothesis : N > 55\n",
        "\n",
        "Test Type: Right Tailed Test"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# Extract the 'Age' column\n",
        "age_data = df['Age']\n",
        "\n",
        "# Null Hypothesis: N <= 55\n",
        "# Alternate Hypothesis: N > 55\n",
        "# Test Type: Right Tailed Test\n",
        "\n",
        "mu = 55  # population mean according to the null hypothesis\n",
        "alpha = 0.05  # significance level\n",
        "\n",
        "# Calculate the sample mean and standard deviation\n",
        "sample_mean = age_data.mean()\n",
        "sample_std = age_data.std(ddof=1)\n",
        "n = len(age_data)\n",
        "\n",
        "# Perform the one-sample t-test\n",
        "t_statistic, p_value = stats.ttest_1samp(age_data, mu)\n",
        "\n",
        "# Since it's a right-tailed test, we adjust the p-value\n",
        "p_value_right_tailed = p_value / 2 if t_statistic > 0 else 1 - (p_value / 2)\n",
        "\n",
        "# Print the results\n",
        "print(\"Sample mean:\", sample_mean)\n",
        "print(\"Sample standard deviation:\", sample_std)\n",
        "print(\"Sample size:\", n)\n",
        "print(\"T-statistic:\", t_statistic)\n",
        "print(\"Right-tailed p-value:\", p_value_right_tailed)\n",
        "\n",
        "# Decision based on the p-value\n",
        "if p_value_right_tailed < alpha:\n",
        "    print(\"Reject the null hypothesis: There is evidence that N > 55.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: There is not enough evidence that N > 55.\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical test used to obtain the p-value is the one-sample t-test. This test compares the mean of a single sample to a known value (in this case, the hypothesized population mean of 55) to determine if there is evidence that the sample mean is significantly different from the hypothesized mean."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Continuous Data: The 'Age' column contains continuous data representing the ages of individuals, making it appropriate for a t-test, which assumes the data is continuous and approximately normally distributed.\n",
        "\n",
        "Sample Size: The sample size is sufficiently large. The Central Limit Theorem ensures that the sampling distribution of the mean will be approximately normal, which justifies the use of the t-test even if the original data is not perfectly normal."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis: N <= 35000\n",
        "\n",
        "Alternate Hypothesis : N > 35000\n",
        "\n",
        "Test Type: Right Tailed Test"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "\n",
        "# Extract the 'Annual_Premium' column\n",
        "annual_premium_data = df['Annual_Premium']\n",
        "\n",
        "# Null Hypothesis: N <= 35000\n",
        "# Alternate Hypothesis: N > 35000\n",
        "# Test Type: Right Tailed Test\n",
        "\n",
        "# Parameters for the test\n",
        "mu = 35000  # population mean according to the null hypothesis\n",
        "alpha = 0.05  # significance level\n",
        "\n",
        "# Calculate the sample mean and standard deviation\n",
        "sample_mean = annual_premium_data.mean()\n",
        "sample_std = annual_premium_data.std(ddof=1)\n",
        "n = len(annual_premium_data)\n",
        "\n",
        "# Perform the one-sample t-test\n",
        "t_statistic, p_value = stats.ttest_1samp(annual_premium_data, mu)\n",
        "\n",
        "# Since it's a right-tailed test, we adjust the p-value\n",
        "p_value_right_tailed = p_value / 2 if t_statistic > 0 else 1 - (p_value / 2)\n",
        "\n",
        "# Print the results\n",
        "print(\"Sample mean:\", sample_mean)\n",
        "print(\"Sample standard deviation:\", sample_std)\n",
        "print(\"Sample size:\", n)\n",
        "print(\"T-statistic:\", t_statistic)\n",
        "print(\"Right-tailed p-value:\", p_value_right_tailed)\n",
        "\n",
        "# Decision based on the p-value\n",
        "if p_value_right_tailed < alpha:\n",
        "    print(\"Reject the null hypothesis: There is evidence that N > 35000.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: There is not enough evidence that N > 35000.\")\n",
        ""
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical test used to obtain the p-value is the one-sample t-test."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, the hypothesis is about comparing the mean of a single sample ('Annual_Premium') against a known value (35000 in the null hypothesis). The one-sample t-test is suitable for this scenario where we are testing whether the mean of the sample is significantly greater than 35000."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Handling Missing Values & Missing Value Imputation\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no missing values."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "# using boxplot to check for outliers\n",
        "\n",
        "for col in numerical_df.columns:\n",
        "    sns.boxplot(numerical_df[col])\n",
        "    plt.title(col)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to remove outliers based on IQR\n",
        "def remove_outliers(data, column_name):\n",
        "    Q1 = data[column_name].quantile(0.25)\n",
        "    Q3 = data[column_name].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # Define boundaries for outliers\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Filter out rows where column value is within the bounds\n",
        "    filtered_data = data[(data[column_name] >= lower_bound) & (data[column_name] <= upper_bound)]\n",
        "\n",
        "    return filtered_data\n",
        "\n",
        "\n",
        "# Remove outliers\n",
        "df_new = remove_outliers(df, 'Annual_Premium')\n",
        "\n",
        "# Display summary before and after outlier removal\n",
        "print(\"Before outlier removal:\")\n",
        "print(df.describe())\n",
        "\n",
        "print(\"\\nAfter outlier removal:\")\n",
        "print(df_new.describe())\n"
      ],
      "metadata": {
        "id": "IEuKCs_YO4C-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Boxplot to find which features have outliers and the Annual Premium is the only feature with outliers in the dataset. So I used IQR technique to remove the outliers which is the robust technique mostly used for any outlier removal problem."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Encode your categorical columns\n",
        "\n",
        "#labelencoding for vehicle damage\n",
        "le = LabelEncoder()\n",
        "df_new['Vehicle_Damage'] = le.fit_transform(df_new['Vehicle_Damage'])\n",
        "\n",
        "df_new['Vehicle_Damage'].value_counts()"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#encoding categorical variables\n",
        "\n",
        "df_new = pd.get_dummies(df_new, columns=['Gender', 'Vehicle_Age'])"
      ],
      "metadata": {
        "id": "eUZu4ro3PGEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_new.head()"
      ],
      "metadata": {
        "id": "pCBnvduAPJI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I used labelencoding for the feature Vehicle_Damage as it have two values(Yes/No). I also used OneHotEncoding for categorical features like Gender and Vehicle_Age, it creates new columns based on the category."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "#Contain all independent variables\n",
        "x = df_new.drop(['Response'], axis=1)\n",
        "\n",
        "#Contain Dependent variable\n",
        "y = df_new['Response']"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x.shape"
      ],
      "metadata": {
        "id": "7N03_GJRPdV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Select your features wisely to avoid overfitting\n",
        "\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "#Implementation Variance Threshold\n",
        "\n",
        "variance_threshold = VarianceThreshold(threshold=0)\n",
        "variance_threshold.fit(df_new)\n",
        "variance_threshold.get_support()"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Implementation ExtraTreesClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "extra_tree_forest = ExtraTreesClassifier(n_estimators = 5,criterion ='entropy', max_features = 2)\n",
        "\n",
        "# Training the model\n",
        "extra_tree_forest.fit(x, y)\n",
        "\n",
        "# Computing the importance of each feature\n",
        "feature_importance = extra_tree_forest.feature_importances_"
      ],
      "metadata": {
        "id": "aAa7Vpi-PkTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing the individual importances\n",
        "feature_importance_normalized = np.std( [ tree.feature_importances_ for tree in extra_tree_forest.estimators_ ] , axis = 0)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "7ES-W1TqPnoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting a Bar Graph to compare the models\n",
        "plt.figure(figsize = (24,10))\n",
        "plt.bar(x.columns, feature_importance_normalized, color='green')\n",
        "plt.xlabel('Feature Labels' , fontsize = 15)\n",
        "plt.ylabel('Feature Importances' , fontsize = 15)\n",
        "plt.title('Comparison of different Feature Importances' , fontsize = 25)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mWXfU8O-PqkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping less important features\n",
        "df_new.drop(columns=['Driving_License','Gender_Female','Gender_Male','id'],inplace=True)"
      ],
      "metadata": {
        "id": "OJ57W-tEPx7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Compute the correlation matrix\n",
        "corr_matrix = df_new.corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title('Pearson Correlation Matrix', fontsize=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1X3V6zVeP0w6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating function to see the Highly correlated variable , using our provided threshold.\n",
        "def correlation(df, threshold):\n",
        "    col_corr = set()\n",
        "    corr_matrix = df.corr()\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i):\n",
        "            if (corr_matrix.iloc[i, j]) > threshold:\n",
        "                colname = corr_matrix.columns[i]\n",
        "                col_corr.add(colname)\n",
        "    return col_corr\n",
        "# Mostly 0.85 is considered for Highly correlated , so its not above 0.85 .\n",
        "corr_features = correlation(df_new, 0.75)\n",
        "len(set(corr_features))\n",
        "\n",
        "print(corr_features)"
      ],
      "metadata": {
        "id": "l_eod3ocP4Wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def remove_highly_correlated_features(data, threshold=0.9):\n",
        "    corr_matrix = data.corr().abs()\n",
        "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
        "    return data.drop(columns=to_drop)\n",
        "\n",
        "data_no_corr = remove_highly_correlated_features(df_new, threshold=0.9)\n",
        "data_no_corr.columns"
      ],
      "metadata": {
        "id": "aiQoJ4isP7x3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used three method for feature selection those are:\n",
        "\n",
        "* Variance threshold\n",
        "* Extra Tree classification\n",
        "* correlation map\n",
        "\n",
        "I used Variance threshold to check the which columns has constant values, Extrs Tree classification for checking which columns are less important for the dependent variable and correlation map for the removing of highly correlated variables."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "'Age', 'Region_Code', 'Previously_Insured', 'Vehicle_Damage','Annual_Premium','Policy_Sales_Channel', 'Vintage', 'Response','Vehicle_Age_1-2 Year','Vehicle_Age_< 1 Year','Vehicle_Age_> 2 Years'\n",
        "\n",
        "These are important features from the above Observation."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n",
        "#contains all independent variables\n",
        "X = df_new.drop(['Response'], axis=1)\n",
        "\n",
        "#contains dependent variable\n",
        "y = df_new['Response']\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"X train Shape: {X_train.shape}\")\n",
        "print(f\"X test Shape: {X_test.shape}\")\n",
        "print(f\"y train Shape: {y_train.shape}\")\n",
        "print(f\"y test Shape: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "fuNbvu0fQ_QT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "I used 80% for training and 20% data for testing as it is optimum to prevent the problem of overfitting or underfitting."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes the dataset is highly imbalanced having negative response of 325634(87.8%) and postive of 45155(12.2%).This shows that the proper handling of imbalanced dataset is needed otherwise the model will predict the most frequent output hence the results will be biased."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "responses = df_new['Response'].value_counts().reset_index()\n",
        "print(responses)\n",
        "\n",
        "# Distribution of dependent variable using pie\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.pie(responses['count'], labels=responses['Response'], autopct='%1.1f%%', colors=sns.color_palette('Set2')[1::-1])\n",
        "plt.title('Response Distribution')\n",
        "plt.axis('equal')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# to handle the imbalanced data i'll use smote(Synthetic Minority Oversampling Technique)\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train, y_train = smote.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "BJVMRAD0RPGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is balanced with smote oversampling technique, its one of the popular technique to handle imbalaced datasets and the dataset can be used for training."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Scaling"
      ],
      "metadata": {
        "id": "eMcLmJZeRbkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Scaling your data\n",
        "# using Standardscaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        ""
      ],
      "metadata": {
        "id": "6bqRaWQjRmDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "sTelV5BdRe_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used StandardScaler method for Data Scaling. StandardScaler helps improve the performance of machine learning models, particularly those that are sensitive to feature scales, such as linear regression, logistic regression, and support vector machines. This is because scaling the features makes it easier for the model to learn the relationships between them."
      ],
      "metadata": {
        "id": "k6nkO-Y_Rrav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 Implementing Logistic Regression"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ML Model - 1 Implementation\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "\n",
        "# Fit the Algorithm\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model class\n",
        "train_pred_log = model.predict(X_train)\n",
        "test_pred_log = model.predict(X_test)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the coefficients\n",
        "model.coef_\n",
        ""
      ],
      "metadata": {
        "id": "JMbWVy83W9Ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the intercept value\n",
        "model.intercept_"
      ],
      "metadata": {
        "id": "HQD8jwgPXAvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating different metrics\n",
        "#calculating metrics for training\n",
        "print(\"For training\")\n",
        "print(classification_report(y_train, train_pred_log))\n",
        "\n",
        "roc_auc_score_train = roc_auc_score(y_train, train_pred_log)\n",
        "print(f\"Train ROC AUC Score: {roc_auc_score_train}\")\n",
        "\n",
        "print(\"-\"*100)\n",
        "\n",
        "#calculating metrics for testing\n",
        "print(\"For testing\")\n",
        "print(classification_report(y_test, test_pred_log))\n",
        "\n",
        "roc_auc_score_test = roc_auc_score(y_test, test_pred_log)\n",
        "print(f\"Test ROC AUC Score: {roc_auc_score_test}\")"
      ],
      "metadata": {
        "id": "fE8AlCGwXE6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Interested', 'Not_Interested']\n",
        "cm = confusion_matrix(y_train, train_pred_log)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Interested', 'Not_Interested']\n",
        "cm = confusion_matrix(y_test, test_pred_log)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "QitTDvgrXQmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Logistic regression algorithm to create the model and the results are as following:\n",
        "\n",
        "For training dataset, i found precision of 87% and recall of 75% and f1-score of 80% for Not Interested customer data. But, I am also interested to see the result for Interested cutomer result as I got precision of 78% and recall of 89% and f1-score of 83%. Accuracy is 82% and average percision, recall & f1_score are 82%, 82% and 82% respectively with a roc auc score of 82%.\n",
        "\n",
        "For testing dataset, i found precision of 96% and recall of 74% and f1-score of 84% for Not Interested customer data. But, I am also interested to see the result for Interested cutomer result as I got precision of 29% and recall of 76% and f1-score of 42%. Accuracy is 75% and average percision, recall & f1_score are 62%, 75% and 63% respectively with a roc auc score of 75%\n",
        "\n",
        "Next tryting to improving the score by using hyperparameter tuning technique."
      ],
      "metadata": {
        "id": "tDkkHwQmXW7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# commentincg this code as it takes so much time but will be using the best parameters given by this gridsearch model\n",
        "'''model = LogisticRegression(max_iter=10000)\n",
        "param_grid = {solvers : ['lbfgs'],\n",
        "penalty : ['10','l2','14','16'],\n",
        "c_values : [ 0.1, 0.01,0.001]}\n",
        "\n",
        "# define grid search\n",
        "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='f1',error_score=0)\n",
        "\n",
        "# Fit the Algorithm\n",
        "grid_result=grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "# Predict on the model'''\n",
        "\n",
        "# Get the predicted classes\n",
        "best_params_log = {'C': 0.001, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
        "grid_result = LogisticRegression(**best_params_log)\n",
        "grid_result.fit(X_train, y_train)\n",
        "grid_train_log_preds = grid_result.predict(X_train)\n",
        "grid_test_log_preds = grid_result.predict(X_test)"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating different metrics\n",
        "#calculating metrics for training\n",
        "print(\"For training\")\n",
        "print(classification_report(y_train, grid_train_log_preds))\n",
        "\n",
        "roc_auc_score_train = roc_auc_score(y_train, grid_train_log_preds)\n",
        "print(f\"Train ROC AUC Score: {roc_auc_score_train}\")\n",
        "\n",
        "print(\"-\"*100)\n",
        "\n",
        "#calculating metrics for testing\n",
        "print(\"For testing\")\n",
        "# Use the test set predictions instead of training set predictions\n",
        "print(classification_report(y_test, grid_test_log_preds))\n",
        "\n",
        "roc_auc_score_test = roc_auc_score(y_test, grid_test_log_preds)\n",
        "print(f\"Test ROC AUC Score: {roc_auc_score_test}\")"
      ],
      "metadata": {
        "id": "TMBozxQwXkQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used GridSearchCV hyperparameter tuning technique. GridSearchCV performs an exhaustive search over a specified parameter grid. This means it tests all possible combinations of the hyperparameters you define. This thorough search can help identify the optimal combination of hyperparameters for your model.GridSearchCV integrates cross-validation into the hyperparameter search. Cross-validation involves partitioning the data into multiple subsets, training the model on some subsets, and validating it on the remaining ones. This helps in assessing the models performance more robustly compared to a single train-test split, ensuring that the chosen hyperparameters generalize well to unseen data.\n",
        "\n",
        "GridSearchCV supports parallelization, allowing you to distribute the search across multiple processors. This can significantly speed up the search process, especially when the parameter grid is large."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training dataset, i found precision of 87% and recall of 74% and f1-score of 80% for Not Interested customer data. But, I am also interested to see the result for Interested cutomer result as I got precision of 77% and recall of 89% and f1-score of 83%. Accuracy is 82% and average percision, recall & f1_score are 82%, 82% and 81% respectively with a roc auc score of 82%.\n",
        "\n",
        "For testing dataset, i found precision of 96% and recall of 74% and f1-score of 83% for Not Interested customer data. But, I am also interested to see the result for Interested cutomer result as I got precision of 29% and recall of 77% and f1-score of 42%. Accuracy is 74% and average percision, recall & f1_score are 62%, 75% and 63% respectively with a roc auc score of 75%\n",
        "\n",
        "There is increase by 1% for recall in testing but no good improvement is seen."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "# Create an instance of the XGBClassifier\n",
        "xg_model = XGBClassifier()\n",
        "\n",
        "# Fit the Algorithm\n",
        "xg_models=xg_model.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "# Making predictions on train and test data\n",
        "train_preds_xg = xg_models.predict(X_train)\n",
        "test_preds_xg = xg_models.predict(X_test)"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating different metrics\n",
        "#calculating metrics for training\n",
        "print(\"For training\")\n",
        "print(classification_report(y_train, train_preds_xg))\n",
        "\n",
        "roc_auc_score_train = roc_auc_score(y_train, train_preds_xg)\n",
        "print(f\"Train ROC AUC Score: {roc_auc_score_train}\")\n",
        "\n",
        "print(\"-\"*100)\n",
        "\n",
        "#calculating metrics for testing\n",
        "print(\"For testing\")\n",
        "print(classification_report(y_test, test_preds_xg))\n",
        "\n",
        "roc_auc_score_test = roc_auc_score(y_test, test_preds_xg)\n",
        "print(f\"Test ROC AUC Score: {roc_auc_score_test}\")"
      ],
      "metadata": {
        "id": "vSUFvdK0X9GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Interested', 'Not_Interested']\n",
        "cm = confusion_matrix(y_train, train_preds_xg)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "Iy3smozSYCb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Interested', 'Not_Interested']\n",
        "cm = confusion_matrix(y_test, test_preds_xg)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)\n",
        ""
      ],
      "metadata": {
        "id": "6wss-FyTYHo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used XGBClassifier algorithm to create the model and the results are as following:\n",
        "\n",
        "For training dataset, i found precision of 89% and recall of 86% and f1-score of 87% for Not Interested customer data. But, I am also interested to see the result for Interested cutomer result as I got precision of 86% and recall of 89% and f1-score of 88%. Accuracy is 88% and average percision, recall & f1_score are 88%, 88% and 88% respectively with a roc auc score of 88%.\n",
        "\n",
        "For testing dataset, i found precision of 93% and recall of 85% and f1-score of 89% for Not Interested customer data. But, I am also interested to see the result for Interested cutomer result as I got precision of 33% and recall of 53% and f1-score of 41%. Accuracy is 81% and average percision, recall & f1_score are 63%, 69% and 65% respectively with a roc auc score of 69%\n",
        "\n",
        "Next tryting to improving the score by using hyperparameter tuning technique."
      ],
      "metadata": {
        "id": "FFE1IZV2YMnb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Define the parameter grid\n",
        "\n",
        "#commenting this code because this code requires so mucch time to run insted we are directly using the best parameters\n",
        "'''param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'subsample': [0.8, 1.0],\n",
        "    'colsample_bytree': [0.8, 1.0]\n",
        "}\n",
        "# Initialize the model\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid,\n",
        "                           scoring='accuracy', cv=3, verbose=1, n_jobs=-1)\n",
        "\n",
        "# Fit the Algorithm\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "train_preds = grid_search.predict(X_train)\n",
        "test_preds = grid_search.predict(X_test)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Score:\", grid_search.best_score_)'''\n",
        "\n",
        "#these are the best parameters\n",
        "#Best Parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 300, 'subsample': 0.8}\n",
        "#Best Score: 0.8620096008732742\n",
        "\n",
        "# we will use these hyper-parameters in XGBClassifier\n",
        "best_params_xg = {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 300, 'subsample': 0.8}\n",
        "xg_model = XGBClassifier(**best_params_xg)\n",
        "\n",
        "# Fit the Algorithm\n",
        "xg_models=xg_model.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "# Making predictions on train and test data\n",
        "train_preds_xg_gr = xg_models.predict(X_train)\n",
        "test_preds_xg_gr = xg_models.predict(X_test)"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating different metrics\n",
        "#calculating metrics for training\n",
        "print(\"For training\")\n",
        "print(classification_report(y_train, train_preds_xg_gr))\n",
        "\n",
        "roc_auc_score_train = roc_auc_score(y_train, train_preds_xg_gr)\n",
        "print(f\"Train ROC AUC Score: {roc_auc_score_train}\")\n",
        "\n",
        "print(\"-\"*100)\n",
        "\n",
        "#calculating metrics for testing\n",
        "print(\"For testing\")\n",
        "print(classification_report(y_test, test_preds_xg_gr))\n",
        "\n",
        "roc_auc_score_test = roc_auc_score(y_test, test_preds_xg_gr)\n",
        "print(f\"Test ROC AUC Score: {roc_auc_score_test}\")"
      ],
      "metadata": {
        "id": "njXbAGVfaeNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used GridSearchCV hyperparameter tuning technique. GridSearchCV performs an exhaustive search over a specified parameter grid. This means it tests all possible combinations of the hyperparameters you define. This thorough search can help identify the optimal combination of hyperparameters for your model.GridSearchCV integrates cross-validation into the hyperparameter search. Cross-validation involves partitioning the data into multiple subsets, training the model on some subsets, and validating it on the remaining ones. This helps in assessing the models performance more robustly compared to a single train-test split, ensuring that the chosen hyperparameters generalize well to unseen data.\n",
        "\n",
        "GridSearchCV supports parallelization, allowing you to distribute the search across multiple processors. This can significantly speed up the search process, especially when the parameter grid is large."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training dataset, i found precision of 90% and recall of 87% and f1-score of 88% for Not Interested customer data. But, I am also interested to see the result for Interested cutomer result as I got precision of 87% and recall of 90% and f1-score of 88%. Accuracy is 88% and average percision, recall & f1_score are 88%, 88% and 88% respectively with a roc auc score of 88%.\n",
        "\n",
        "For testing dataset, i found precision of 93% and recall of 86% and f1-score of 89% for Not Interested customer data. But, I am also interested to see the result for Interested cutomer result as I got precision of 34% and recall of 52% and f1-score of 41%. Accuracy is 82% and average percision, recall & f1_score are 63%, 69% and 65% respectively with a roc auc score of 69%\n",
        "\n",
        "After Tuning almost all metrics have increased by 1% it shows a positive increament."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "# Create an instance of the RandomForestClassifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Fit the Algorithm\n",
        "rf_model.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "# Making predictions on train and test data\n",
        "train_preds_rf = rf_model.predict(X_train)\n",
        "test_preds_rf = rf_model.predict(X_test)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating different metrics\n",
        "#calculating metrics for training\n",
        "print(\"For training\")\n",
        "print(classification_report(y_train, train_preds_rf))\n",
        "\n",
        "roc_auc_score_train = roc_auc_score(y_train, train_preds_rf)\n",
        "print(f\"Train ROC AUC Score: {roc_auc_score_train}\")\n",
        "\n",
        "print(\"-\"*100)\n",
        "\n",
        "#calculating metrics for testing\n",
        "print(\"For testing\")\n",
        "print(classification_report(y_test, test_preds_rf))\n",
        "\n",
        "roc_auc_score_test = roc_auc_score(y_test, test_preds_rf)\n",
        "print(f\"Test ROC AUC Score: {roc_auc_score_test}\")\n",
        ""
      ],
      "metadata": {
        "id": "OQwNRfFaa2M0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Interested', 'Not_Interested']\n",
        "cm = confusion_matrix(y_train, train_preds_rf)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Interested', 'Not_Interested']\n",
        "cm = confusion_matrix(y_test, test_preds_rf)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "uvhNMtYGbFRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Random Forest algorithm. This is one of the alogorithms where the model learns from weak learners. But here I got to see overfitting.\n",
        "\n",
        "For training dataset, i found precision of 100% and recall of 100% and f1-score of 100% for Not Interested customer. But, I also wanted to see the result for Interested customer, I got precision of 100% and recall of 100% and f1-score of 100%. Accuracy is 100% and average percision, recall & f1_score are 100%, 100% and 100% respectively with a roc auc score of 100%. This clearly shows that the model is overfitting and it must be handled.\n",
        "\n",
        "For testing dataset, i found precision of 91% and recall of 89% and f1-score of 90% for Not Interested customer. But, I also wanted to see the result for Interested customer, I got precision of 32% and recall of 38% and f1-score of 35%. Accuracy is 83% and average percision, recall & f1_score are 62%, 63% and 62% respectively with a roc auc score of 63%.\n",
        "\n",
        "Next tryting to improving the score by using hyperparameter tuning technique."
      ],
      "metadata": {
        "id": "Hl4ybCW_bIon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# commenting this code as takes too much time to execute but we will use the best parameters estimated from this process\n",
        "'''\n",
        "# Number of trees\n",
        "n_estimators = [50,80,100]\n",
        "\n",
        "# Maximum depth of trees\n",
        "max_depth = [4,6,8]\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [50,100,150]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [40,50]\n",
        "\n",
        "# HYperparameter Grid\n",
        "param_dict = {'n_estimators' : n_estimators,\n",
        "              'max_depth' : max_depth,\n",
        "              'min_samples_split' : min_samples_split,\n",
        "              'min_samples_leaf' : min_samples_leaf}\n",
        "\n",
        "# Create an instance of the RandomForestClassifier\n",
        "rf_model = RandomForestClassifier()\n",
        "\n",
        "# Grid search\n",
        "rf_grid = GridSearchCV(estimator=rf_model,\n",
        "                       param_grid = param_dict,\n",
        "                       cv = 5, verbose=2, scoring='f1')\n",
        "\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "rf_grid.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "train_preds_rf_gs = rf_grid.predict(X_train)\n",
        "test_preds_rf_gs = rf_grid.predict(X_test)\n",
        "\n",
        "print(\"Best: %f using %s\" % (rf_rs.best_score_, rf_rs.best_params_))\n",
        "'''\n",
        "# using best params predicted by gridsearch\n",
        "best_params_rf = {'max_depth': 8, 'min_samples_leaf': 40, 'min_samples_split': 50, 'n_estimators': 80}\n",
        "\n",
        "rf_grid = RandomForestClassifier(**best_params_rf, random_state=42)\n",
        "\n",
        "# Fit the Algorithm\n",
        "rf_grid.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "# Making predictions on train and test data\n",
        "train_preds_rf_gs = rf_grid.predict(X_train)\n",
        "test_preds_rf_gs = rf_grid.predict(X_test)"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating different metrics\n",
        "#calculating metrics for training\n",
        "print(\"For training\")\n",
        "print(classification_report(y_train, train_preds_rf_gs))\n",
        "\n",
        "roc_auc_score_train = roc_auc_score(y_train, train_preds_rf_gs)\n",
        "print(f\"Train ROC AUC Score: {roc_auc_score_train}\")\n",
        "\n",
        "print(\"-\"*100)\n",
        "\n",
        "#calculating metrics for testing\n",
        "print(\"For testing\")\n",
        "print(classification_report(y_test, test_preds_rf_gs))\n",
        "\n",
        "roc_auc_score_test = roc_auc_score(y_test, test_preds_rf_gs)\n",
        "print(f\"Test ROC AUC Score: {roc_auc_score_test}\")"
      ],
      "metadata": {
        "id": "JhKVPS8mbSGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used GridSearchCV hyperparameter tuning technique. GridSearchCV performs an exhaustive search over a specified parameter grid. This means it tests all possible combinations of the hyperparameters you define. This thorough search can help identify the optimal combination of hyperparameters for your model.GridSearchCV integrates cross-validation into the hyperparameter search. Cross-validation involves partitioning the data into multiple subsets, training the model on some subsets, and validating it on the remaining ones. This helps in assessing the models performance more robustly compared to a single train-test split, ensuring that the chosen hyperparameters generalize well to unseen data.\n",
        "\n",
        "GridSearchCV supports parallelization, allowing you to distribute the search across multiple processors. This can significantly speed up the search process, especially when the parameter grid is large."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training dataset, i found precision of 91% and recall of 74% and f1-score of 81% for Not Interested customer data. But, I am also interested to see the result for Interested cutomer as I got precision of 78% and recall of 93% and f1-score of 85%. Accuracy is 83% and average percision, recall & f1_score are 84%, 83% and 83% respectively with a roc auc score of 83%.\n",
        "\n",
        "For testing dataset, i found precision of 97% and recall of 74% and f1-score of 84% for Not Interested customer data. But, I am also interested to see the result for Interested cutomer as I got precision of 30% and recall of 81% and f1-score of 44%. Accuracy is 75% and average percision, recall & f1_score are 63%, 77% and 64% respectively with a roc auc score of 77%\n",
        "\n",
        "After Tuning almost the recall of testing dataset interested customer increased significantly, it incresed by (43%) and this is a crucial metric for our prediction, also there is improvement in f1 score as well."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a revised version of your explanation:\n",
        "\n",
        "This is a classification model where we aim to predict whether a customer will buy vehicle insurance. Initially, the dataset was imbalanced, with more samples indicating customers not interested (0) in purchasing vehicle insurance. This imbalance was likely to bias the model's performance.\n",
        "\n",
        "To address this, I used SMOTE (Synthetic Minority Over-sampling Technique) to balance the data. However, synthetic values do not fully represent real-world scenarios. The problem statement requires us to determine customer interest for the company. Even after applying SMOTE, the dataset may still not accurately reflect real-world data. Therefore, accuracy is not a reliable metric for this imbalanced dataset.\n",
        "\n",
        "Instead, we focus on precision and recall. Precision is the ratio of true positives to the total number of predicted positives, while recall is the ratio of true positives to the sum of true positives and false negatives. We prioritize recall because we do not want the model to falsely predict that a customer is not interested when they actually are, as this could lead to losing potential customers. Thus, recall is our most important metric, followed by F1-score and precision"
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From all the models evaluated, I selected the RandomForestClassifier as final prediction model as it provided the most desired results. Its performance metrics are as follows: Accuracy Score is 0.75, Precision is 0.63, Recall is 0.78, F1 Score is 0.64, and ROC AUC Score is 0.79. These metrics represent the best results compared to the other models. Since recall is the prioritized metric and this model has the highest recall value among all, the RandomForestClassifier is the most suitable choice.\n",
        "\n",
        "The plots for each model and their corresponding metrics are provided below:."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_df = pd.DataFrame({'Model':['Logistic Regression','XGBClassifier','RandomForestClassifier'],\n",
        "                           'Precision_score_%':[62,63,63],\n",
        "                           'Recall_score_%':[75,69,77],\n",
        "                           'F1_score_%':[63, 65, 64],\n",
        "                           'roc_auc_score_%':[75,69,78]})\n",
        "\n",
        "# Melt the DataFrame to long format for seaborn\n",
        "metrics_melted = metrics_df.melt(id_vars='Model', var_name='Metric', value_name='Score')\n",
        "\n",
        "# Create a color palette\n",
        "palette = sns.color_palette(\"rocket\", len(metrics_melted['Metric'].unique()))\n",
        "\n",
        "# Create the bar plot\n",
        "plt.figure(figsize=(10, 7))\n",
        "ax=sns.barplot(data=metrics_melted, x='Model', y='Score', hue='Metric', palette=palette)\n",
        "\n",
        "# Add a horizontal line at the threshold value\n",
        "threshold = 78\n",
        "ax.axhline(threshold, color='black', linewidth=1, linestyle='--')\n",
        "\n",
        "# Add titles and labels\n",
        "plt.title('Comparison of Performance Metrics Across Models', fontsize=16)\n",
        "plt.xlabel('Model', fontsize=14)\n",
        "plt.ylabel('Score (%)', fontsize=14)\n",
        "plt.legend(title='Metric', fontsize=10)\n",
        "plt.ylim(0, 100)\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hOoioHXybtGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_plot(importances):\n",
        "\n",
        "    # Display the five most important features\n",
        "    indices = np.argsort(importances)[::-1]\n",
        "    columns = X.columns.values[indices[:5]]\n",
        "    values = importances[indices][:5]\n",
        "\n",
        "    # Creat the plot\n",
        "    fig = plt.figure(figsize = (9,5))\n",
        "    plt.title(\"Normalized Weights for First Five Most Predictive Features\", fontsize = 16)\n",
        "    plt.bar(np.arange(5), values, width = 0.2, align=\"center\", color = '#74c69d', \\\n",
        "          label = \"Feature Weight\")\n",
        "    plt.bar(np.arange(5) - 0.2, np.cumsum(values), width = 0.2, align = \"center\", color = '#002855', \\\n",
        "          label = \"Cumulative Feature Weight\")\n",
        "    plt.xticks(np.arange(5), columns)\n",
        "    plt.xlim((-0.5, 4.5))\n",
        "    plt.ylabel(\"Weight\", fontsize = 12)\n",
        "    plt.xlabel(\"Feature\", fontsize = 12)\n",
        "\n",
        "    plt.legend(loc = 'upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2VZWnPnVcByd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_feature_importance():\n",
        "    model = RandomForestClassifier(**best_params_rf, random_state=42)\n",
        "    model.fit(X_train,y_train)\n",
        "\n",
        "    importances = np.mean([\n",
        "        tree.feature_importances_ for tree in model.estimators_\n",
        "        ], axis=0)\n",
        "    feature_plot(importances)\n",
        "\n",
        "\n",
        "show_feature_importance()"
      ],
      "metadata": {
        "id": "xz3TUVgDcDGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the RandomForestClassifier as the predictive model because it generally provides good performance in terms of accuracy and robustness against overfitting due to its ensemble nature. A Random Forest model creates a multitude of decision trees during training and outputs the mode of the classes (classification) of the individual trees.\n",
        "\n",
        "For explaining the model and understanding the importance of each feature, I used a model explainability tool such as SHAP (SHapley Additive exPlanations) or feature importance scores provided directly by the RandomForestClassifier.\n",
        "\n",
        "Feature Importance Analysis Based on the RandomForestClassifier, the top 5 features contributing to the prediction of whether a customer will buy vehicle insurance are:\n",
        "\n",
        "1.Previously_Insured: This feature has the highest importance, indicating that whether a customer has been previously insured plays a crucial role in predicting their interest in vehicle insurance.\n",
        "\n",
        "2.Vehicle_Damage: This feature also has significant importance, showing that customers who have had vehicle damage are more likely to consider purchasing vehicle insurance.\n",
        "\n",
        "3.Vehicle_Age_1-2_Year: The age of the vehicle, specifically in the range of 1-2 years, is a key factor. Newer vehicles might have different insurance needs compared to older vehicles.\n",
        "\n",
        "4.Age: The age of the customer is another important feature, affecting their likelihood to buy vehicle insurance.\n",
        "\n",
        "5.Policy_Sales_Channel: This feature represents the channel through which the policy was sold and holds significant weight in the prediction, reflecting the effectiveness of different sales channels.\n",
        "\n",
        "These top 5 features collectively contribute to approximately 90% of the model's predictive power."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Project Conclusion:**\n",
        "\n",
        "The goal of this project was to build a robust classification model to predict whether policyholders with health insurance would be interested in purchasing vehicle insurance. The dataset provided included various demographic, vehicle, and policy details. Through a comprehensive process of data exploration, preprocessing, model training, and evaluation, we developed a model that effectively meets the business objective.\n",
        "\n",
        "####**Key Steps and Findings**\n",
        "\n",
        "#####**Exploratory Data Analysis (EDA)**\n",
        "\n",
        "* Visualization: Histograms, pie charts, bar plots, and heatmaps were used to understand the data distribution and relationships.\n",
        "\n",
        "* Imbalance Identification: The dataset was found to be imbalanced, with more policyholders not interested in vehicle insurance.\n",
        "\n",
        "#####**Data Preprocessing**\n",
        "\n",
        "* Balancing the Dataset: Applied SMOTE (Synthetic Minority Over-sampling Technique) to address the class imbalance.\n",
        "\n",
        "* Categorical Handling: Converted categorical variables using pd.get_dummies.\n",
        "\n",
        "* Feature Scaling: Ensured feature scaling after applying SMOTE and before training the model to standardize the data.\n",
        "\n",
        "##### **Model Selection and Training**\n",
        "\n",
        "* Chosen Model: RandomForestClassifier was selected for its robustness and performance.\n",
        "##### **Hyperparameter Tuning**\n",
        "\n",
        "* Optimization: Conducted hyperparameter tuning using GridSearchCV to enhance the models performance.\n",
        "\n",
        "* Key Parameters: Focused on important hyperparameters to fine-tune the RandomForestClassifier.\n",
        "\n",
        "##### **Evaluation Metrics**\n",
        "\n",
        "* Metric Selection: Prioritized recall over other metrics due to the business need to minimize false negatives (predicting a customer is not interested when they actually are). F1-score and precision were also considered important, while accuracy was deprioritized due to the imbalanced nature of the data.\n",
        "##### **Key Findings**\n",
        "\n",
        "* Feature Importance: The most predictive features were Previously_Insured, Vehicle_Damage, Vehicle_Age (1-2 years), Age, and Policy_Sales_Channel.\n",
        "\n",
        "* Model Performance: The RandomForest model, after hyperparameter tuning, demonstrated improved performance in identifying customers interested in vehicle insurance. The model's high recall ensures that potential customers are less likely to be missed, aligning well with the business objective.\n",
        "\n",
        "The developed model successfully predicts the likelihood of a customer purchasing vehicle insurance, considering the imbalanced nature of the data. By focusing on recall, the model minimizes the risk of missing potential customers, thereby assisting the company in strategizing their communication and marketing efforts more effectively."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}